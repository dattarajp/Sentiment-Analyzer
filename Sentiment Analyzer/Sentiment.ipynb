{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T12:04:39.676619Z",
     "start_time": "2024-02-11T12:04:39.566495Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Sentiment: ## made a class named sentiment which has var for positive, negative and neutral stored.\n",
    "    NEGATIVE=\"NEGATIVE\"\n",
    "    NEUTRAL=\"NEUTRAL\"\n",
    "    POSITIVE=\"POSTIVE\"\n",
    "    \n",
    "class Review: ## made a data class to make it easy to access the review and the score given with .text nad .score\n",
    "    def __init__(self,text,score):\n",
    "        self.text=text\n",
    "        self.score=score\n",
    "        self.sentiment=self.get_sentiments()\n",
    "        \n",
    "        \n",
    "    def get_sentiments(self): ## this will tell us the sentiments on stars/score given \n",
    "        if self.score <= 2:\n",
    "            return Sentiment.NEGATIVE\n",
    "        elif self.score ==3:\n",
    "            return Sentiment.NEUTRAL\n",
    "        else:\n",
    "            return Sentiment.POSITIVE\n",
    "        \n",
    "        \n",
    "class ReviewContainer:\n",
    "    def __init__(self,reviews):\n",
    "        self.reviews=reviews\n",
    "        \n",
    "    def get_text(self):\n",
    "        return [x.text for x in self.reviews]\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        return [x.sentiment for x in self.reviews]\n",
    "\n",
    "\n",
    "    def evenly_distributed(self):\n",
    "        negative= list(filter(lambda x : x.sentiment==Sentiment.NEGATIVE,self.reviews)) ##we are keeping track of the all the negative comments we can do the same thing for positive\n",
    "    \n",
    "        positive=list(filter(lambda x : x.sentiment==Sentiment.POSITIVE,self.reviews))\n",
    "        \n",
    "        positive_shrunk=positive[:len(negative)]\n",
    "        self.reviews=negative+positive_shrunk ##we are just shrunking the amount of reviews storing which only has positive and negative which are equal to the amount of negative\n",
    "        \n",
    "        random.shuffle(self.reviews)\n",
    "        \n",
    "               \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T12:04:40.065104Z",
     "start_time": "2024-02-11T12:04:39.660114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'POSTIVE'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_name='./Books_small_10000.json' ## saved the review file as file_name\n",
    "\n",
    "reviews=[] ## this is an empty array\n",
    "with open(file_name)as f: ##open the file and now its called f\n",
    "    for line in f:\n",
    "       review=json.loads(line) ##at this line we loaded the file with json package\n",
    "       reviews.append(Review(review['reviewText'], review['overall'])) ## we add all the reviews in thie empty array\n",
    "reviews[5].sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREP DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T12:04:42.981354Z",
     "start_time": "2024-02-11T12:04:40.002624Z"
    }
   },
   "outputs": [],
   "source": [
    "## Split our 1000 reviews in traning and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "training,test=train_test_split(reviews,test_size=0.33,random_state=42)##At this stage we are splittinh our 1000 review in test and training sets \n",
    "train_container= ReviewContainer(training)\n",
    "test_container= ReviewContainer(test)\n",
    "\n",
    "##this will return x and y currently which are training as x and test as y test_size is 0.33 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T12:04:43.061159Z",
     "start_time": "2024-02-11T12:04:42.998101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n",
      "436\n"
     ]
    }
   ],
   "source": [
    "train_container.evenly_distributed()\n",
    "train_x=train_container.get_text() ##first we splited our training data into x and y.we use file comprehension to just get the text we did this\n",
    "train_y=train_container.get_sentiment() ##and for train_y which has only sentiment \n",
    "\n",
    "test_container.evenly_distributed()\n",
    "test_x=test_container.get_text() ##same as trainig set we split and did file comprehension\n",
    "test_y=test_container.get_sentiment()\n",
    "\n",
    "print(train_y.count(Sentiment.POSITIVE))\n",
    "print(train_y.count(Sentiment.NEGATIVE)) \n",
    "##now we have equal number of negative and positive sentiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T12:04:43.272704Z",
     "start_time": "2024-02-11T12:04:43.022006Z"
    }
   },
   "outputs": [],
   "source": [
    "##now we will fit our bag_of_words model to the training set and then we will build a classifier on that training set\n",
    "##in order to perform machine learning on text documents we first need to turn text content into numerical feature vectors.\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "vectorizer=TfidfVectorizer() ## this method will turn your text intup into numerical feature vectors check more about bag of words on google.\n",
    "train_x_vectors=vectorizer.fit_transform(train_x) ##fit_transform needs the train_x var which has all the text data.\n",
    "test_x_vectors=vectorizer.transform(test_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T12:04:43.916869Z",
     "start_time": "2024-02-11T12:04:43.284971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['POSTIVE'], dtype='<U8')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm=svm.SVC(kernel='linear')\n",
    "clf_svm.fit(train_x_vectors,train_y) ##Fitting the classifier to the train x any y set\n",
    "\n",
    "test_x[0]\n",
    "clf_svm.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T12:04:44.135517Z",
     "start_time": "2024-02-11T12:04:43.922328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.8076923076923077"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm.score(test_x_vectors,test_y)##82% Accuracy is the score of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### we equally distributed the numbers of positive nad negative comments an then checked the F1 score  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-11T12:04:44.362777Z",
     "start_time": "2024-02-11T12:04:44.139691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.80582524, 0.80952381])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##F1 SCORES\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(test_y,clf_svm.predict(test_x_vectors),average=None,labels=[Sentiment.POSITIVE,Sentiment.NEGATIVE])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array(['POSTIVE', 'NEGATIVE', 'NEGATIVE'], dtype='<U8')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set=['I love you','bad book dont buy it','horrible waste of time']\n",
    "new_test=vectorizer.transform(test_set) ##we transformed the test_set here\n",
    "\n",
    "clf_svm.predict(new_test) ##and then we predicted the sentiments\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T12:04:44.401699Z",
     "start_time": "2024-02-11T12:04:44.366602Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Saving your model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./Models/sentiment_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(clf_svm,f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T12:04:44.426234Z",
     "start_time": "2024-02-11T12:04:44.392922Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('./Models/sentiment_classifier.pkl', 'rb') as f:\n",
    "    loaded_clf=pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T12:06:27.443131Z",
     "start_time": "2024-02-11T12:06:27.409809Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoyed this book very much.  The plot was interesting and the characters were persons one could identify with.  Hard to put down once you start reading.\n"
     ]
    },
    {
     "data": {
      "text/plain": "array(['POSTIVE'], dtype='<U8')"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_x[0])\n",
    "loaded_clf.predict(test_x_vectors[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T12:07:19.987905Z",
     "start_time": "2024-02-11T12:07:19.936011Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
